# -*- coding: utf-8 -*-
"""mini_projet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17KlQ34bgcYhfWOJPIZ1CAdNnhbw9BkJS
"""

from nltk.tokenize import sent_tokenize, word_tokenize

from math import log

from nltk.stem import PorterStemmer

import nltk

from nltk.corpus import stopwords

from google.colab import drive

import nltk

import os

nltk.download("book")

drive.mount('/content/drive')

def raciner(l):
  ps = PorterStemmer()
  a=[]
  for i in l:
    a.append(ps.stem(i))
  return a

def filter(l):
  s=set(stopwords.words("english"))
  a=[]
  for i in l:
    if not i in s:
      a.append(i) 
  return a

def nb_occ(x,l):
  all_words=nltk.FreqDist(l)
  return all_words[x]

def poid(x,l,nb_doc,y):
  return (1+log(nb_occ(x,l)))*log(nb_doc/y)

def doc_retourne(x,l):
  res={}
  for k,v in l.items():
    if nb_occ(x,v)>0:
      res[k]=v
  return res

def best_doc(x,l,nb_doc,y):
  doc_poid={}
  for i in l.keys():
    doc_poid[i]=poid(x,l[i],nb_doc,y)
  maxv=max(doc_poid.values())
  for key, val in doc_poid.items():
    if val==maxv:
      return key

def contien(l,x):
  cont=0
  for i in l.values():
    if x in i:
      cont+=1
  return cont

def tf_idf():
  mytexts = nltk.TextCollection([filtered_sentence, word_tokens])

l=os.listdir('/content/drive/MyDrive/tp_index')
f_txt=[]
for i in l:
  if i[-3:]=="txt":
    f_txt.append(i)
nb_doc=len(f_txt)
x=input()
x1=x
ps1 = PorterStemmer()
x=ps1.stem(x)
txt={}
for i in f_txt:
  a="/content/drive/MyDrive/tp_index/"+i
  f=open(a,'r')
  txt[i]=word_tokenize(f.read())
for i in txt.keys():
  l1=filter(txt[i])
  txt[i]=raciner(txt[i])
txt=doc_retourne(x,txt)
if len(txt)>0:
  y=contien(txt,x)
  for i in txt.keys():
    print("documen {} nombre d occurance du mot {} avec un poid de {}".format(i,nb_occ(x,txt[i]),poid(x,txt[i],nb_doc,y)))   
  print("document le plus pertinent: {}".format(best_doc(x,txt,nb_doc,y)))
else:
  print("n'existe pas")